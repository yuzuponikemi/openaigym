{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ikemix\\anaconda3\\envs\\rl_env\\lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 手書き数字の画像データMNISTをダウンロード\n",
    " \n",
    " \n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "mnist_X, mnist_y = fetch_openml('mnist_784', version=1, data_home=\".\", return_X_y=True)\n",
    "\n",
    "x = mnist_X.astype(np.float32) / 255\n",
    "y = mnist_y.astype(np.int32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここからPyTorchによるディープラーニングを実装します。今回は最低限の実装であるミニマムバージョンで解説します。PyTorchによるディープラーニングの実装は次の6ステップで行われます。\n",
    "\n",
    "データの前処理\n",
    "\n",
    "DataLoderの作成\n",
    "\n",
    "ネットワークの構築\n",
    "\n",
    "誤差関数と最適化手法の設定\n",
    "\n",
    "学習と推論の設定\n",
    "\n",
    "学習と推論の実行\n",
    "\n",
    "データの前処理では、データをニューラルネットワークに投入できるように加工します。今回のMNISTデータの場合は、まずダウンロードしたデータを画像Xとラベルy（0-9）に分けて格納します。画像データはグレースケールの0-255の数値で表現されているので、255で割って0-1になるように正規化します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# MNISTのデータの1つ目を可視化する\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    " \n",
    "plt.imshow(X[0].reshape(28, 28), cmap='gray')\n",
    "print(\"この画像データのラベルは{:.0f}です\".format(y[0]))\n",
    "\n",
    "\"\"\"すると以下の図12.3の結果が出力されます。\n",
    "変数Xには縦28×横28ピクセルの784の要素を持つNumpy形式のベクトルが7万画像分格納されています。\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて正規化したMNISTデータをPyTorchのニューラルネットワークで扱えるDataLoaderという変数へと変換します。\n",
    "\n",
    "　DataLoaderへの変換は以下の4つの手続きからなります。\n",
    "\n",
    "2.1. 訓練データとテストデータに分離\n",
    "2.2. NumPyデータをTensorに変換\n",
    "2.3. Datasetの作成\n",
    "2.4. DatasetをDataLoderに変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3539852970.py, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 24\u001b[1;36m\u001b[0m\n\u001b[1;33m    loader_test = DataLoader(ds_test, batch_size=, shuffle=False)\u001b[0m\n\u001b[1;37m                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 2. DataLoderの作成\n",
    " \n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    " \n",
    "# 2.1 データを訓練とテストに分割（6:1）\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=1/7, random_state=0)\n",
    " \n",
    "# 2.2 データをPyTorchのTensorに変換\n",
    "X_train = torch.Tensor(X_train)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    " \n",
    "# 2.3 データとラベルをセットにしたDatasetを作成\n",
    "ds_train = TensorDataset(X_train, y_train)\n",
    "ds_test = TensorDataset(X_test, y_test)\n",
    " \n",
    "# 2.4 データセットのミニバッチサイズを指定した、Dataloaderを作成\n",
    "# Chainerのiterators.SerialIteratorと似ている\n",
    "loader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "loader_test = DataLoader(ds_test, batch_size=, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. ネットワークの構築\n",
    "# Keras風の書き方 Define and Run\n",
    " \n",
    "from torch import nn\n",
    " \n",
    "model = nn.Sequential()\n",
    "model.add_module('fc1', nn.Linear(28*28, 100))\n",
    "model.add_module('relu1', nn.ReLU())\n",
    "model.add_module('fc2', nn.Linear(100, 100))\n",
    "model.add_module('relu2', nn.ReLU())\n",
    "model.add_module('fc3', nn.Linear(100, 10))\n",
    " \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m optim\n\u001b[0;32m      5\u001b[0m \u001b[39m# 誤差関数の設定\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()  \u001b[39m# 変数名にはcriterionも使われる\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# 重みを学習する際の最適化手法の選択\u001b[39;00m\n\u001b[0;32m      9\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. 誤差関数と最適化手法の設定\n",
    " \n",
    "from torch import optim\n",
    " \n",
    "# 誤差関数の設定\n",
    "loss_fn = nn.CrossEntropyLoss()  # 変数名にはcriterionも使われる\n",
    " \n",
    "# 重みを学習する際の最適化手法の選択\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 学習と推論の設定\n",
    "# 5-1. 学習1回でやることを定義します\n",
    "# Chainerのtraining.Trainer()に対応するものはない\n",
    " \n",
    "from torch.autograd import Variable\n",
    " \n",
    " \n",
    "def train(epoch):\n",
    "    model.train()  # ネットワークを学習モードに切り替える\n",
    " \n",
    "    # データローダーから1ミニバッチずつ取り出して計算する\n",
    "    for data, target in loader_train:\n",
    "        data, target = Variable(data), Variable(target)  # 微分可能に変換\n",
    "        optimizer.zero_grad()  # 一度計算された勾配結果を0にリセット\n",
    " \n",
    "        output = model(data)  # 入力dataをinputし、出力を求める\n",
    "        loss = loss_fn(output, target)  # 出力と訓練データの正解との誤差を求める\n",
    "        loss.backward()  # 誤差のバックプロパゲーションを求める\n",
    "        optimizer.step()  # バックプロパゲーションの値で重みを更新する\n",
    " \n",
    "    print(\"epoch{}：終了\\n\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 学習と推論の設定\n",
    "# 5-2. 推論1回でやることを定義します\n",
    "# Chainerのtrainer.extend(extensions.Evaluator())に対応するものはない\n",
    " \n",
    " \n",
    "def test():\n",
    "    model.eval()  # ネットワークを推論モードに切り替える\n",
    "    correct = 0\n",
    " \n",
    "    # データローダーから1ミニバッチずつ取り出して計算する\n",
    "    for data, target in loader_test:\n",
    "        data, target = Variable(data), Variable(target)  # 微分可能に変換\n",
    "        output = model(data)  # 入力dataをinputし、出力を求める\n",
    " \n",
    "        # 推論する\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # 出力ラベルを求める\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()  # 正解と一緒だったらカウントアップ\n",
    " \n",
    "    # 正解率を出力\n",
    "    data_num = len(loader_test.dataset)  # データの総数\n",
    "    print('\\nテストデータの正解率: {}/{} ({:.0f}%)\\n'.format(correct,\n",
    "                                                   data_num, 100. * correct / data_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習なしにテストデータで推論してみよう\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
